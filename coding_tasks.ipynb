{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Name: Pallavi Dhakne\n",
        "Roll No:BT20CSE201"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76qUcej0Zqb2"
      },
      "source": [
        "# Task 1 : Building a basic GPT-2 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwbomCnXjFpd"
      },
      "source": [
        "Start by implementing the `GPT2-small` model (with 125 million parameters) using Python and PyTorch. Make sure you touch upon the key aspects of the model like multi-head self-attention mechanism, feed-forward networks and positional encoding.\n",
        "\n",
        "Key points:\n",
        "\n",
        "- Follow the original GPT-2 design of using both token and positional embeddings.\n",
        "- Implement the transformer layers with multi-head self-attention and point-wise feed-forward network.\n",
        "- You're required to abstain from using pre-built transformer libraries.\n",
        "\n",
        "Refer to the GPT-2 paper's architecture descriptions in Sections 1 and 2 for further help. ([GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)). Additionally, a great resource could be Andrej Karpathyâ€™s [nanogpt](https://github.com/karpathy/nanoGPT) repository and the [makemore](https://youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&feature=shared) series.\n",
        "\n",
        "To validate your implementation, load the original GPT-2 125M model checkpoints and run a sample prediction.\n",
        "\n",
        "**Deliverable:** Complete Python code featuring the GPT-2 model along with demonstration of appropriate testing to verify its functioning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Firsly we setup the import module\n",
        "torch install using  \n",
        "pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP89OtzOZB2f"
      },
      "source": [
        "**Positional Encoding:**\n",
        "\n",
        "Generating positional encodings to provide information about the position of tokens in the sequence. I am using sine and cosine functions to create these encodings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSLnw6vAZKX0"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "        self.register_buffer('encoding', self.encoding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoding[:, :x.size(1)].detach() + x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIYm1N-5ZQtV"
      },
      "source": [
        "**MultiHead Attention:**\n",
        "\n",
        "Implementing the multi-head self-attention mechanism using linear layers to project the input embeddings into query, key, and value vectors. I am Applying attention mechanisms to these vectors and combine the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LSRfwpEZXV8"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.output = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.split_heads(self.query(q), batch_size)\n",
        "        k = self.split_heads(self.key(k), batch_size)\n",
        "        v = self.split_heads(self.value(v), batch_size)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention, v)\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.output(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpiwktZ5ZbQs"
      },
      "source": [
        "**Feed Forward Networks:**\n",
        "\n",
        "Constructing feed-forward neural networks consisting of multiple layers of linear transformations with non-linear activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp5GR_wTZlaN"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_2(F.relu(self.linear_1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7E302PnZ0zY"
      },
      "source": [
        "**Transformer Layer:**\n",
        "\n",
        "Combining the above components in a transformer layer where you apply multi-head self-attention followed by feed-forward networks and layer normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxFHWenuZ55O"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attention_output = self.self_attention(x, x, x, mask)\n",
        "        x = self.layer_norm_1(x + attention_output)\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.layer_norm_2(x + ff_output)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTz9Jl_BaNyO"
      },
      "source": [
        "**Usages Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq_4KI9XaTVt"
      },
      "outputs": [],
      "source": [
        "# Usage example\n",
        "seq_length = 50\n",
        "batch_size = 16\n",
        "vocab_size = 10000\n",
        "embedding_size = 256\n",
        "num_heads = 8\n",
        "hidden_size = 512\n",
        "\n",
        "# Create input tensor\n",
        "input_data = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
        "embedding = nn.Embedding(vocab_size, embedding_size)(input_data)\n",
        "pos_encoding = PositionalEncoding(embedding_size)(embedding)\n",
        "\n",
        "transformer_layer = TransformerLayer(embedding_size, num_heads, hidden_size)\n",
        "output = transformer_layer(pos_encoding)\n",
        "\n",
        "print(output.shape)  # Check the output shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZZ-QSUCaxHt"
      },
      "source": [
        "#Task 2:Transformer Architectural Changes\n",
        "\n",
        "In the second task, you are required to add alterations to your original GPT-2 model architecture to experiment and assess the potential of improvements. Here's what you need to do:\n",
        "\n",
        "- **Rotary Positional Embedding:** Replace the original positional embeddings in the GPT-2 model with Rotary embeddings. You may refer to [Su et. al. RoFormer](https://arxiv.org/pdf/2104.09864.pdf).\n",
        "- **Group Query Attention:** Equip your model with the Group Query Attention mechanism following the insights from the [Ainslie et. al. GQA: Training Generalized Multi-Query Transformer](https://arxiv.org/pdf/2305.13245v2.pdf). Analyze how this mechanism can modify the model's operation compared to the standard attention mechanism.\n",
        "- **Sliding Window Attention:** Imbibe the Sliding Window Attention mechanism in your model and observe its effects on model performance. Refer to the work by [Beltagy et. al. Longformer](https://arxiv.org/pdf/2004.05150v2.pdf) for better comprehension of its implementation and advantages.\n",
        "\n",
        "**Deliverable:** Python code with any one, two or all three changes. Comment on the model size and capabilities, potential pitfalls and/or any improvement after each change. Points will be awarded for any combination of successful implementations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_5iMfqCmNtP"
      },
      "source": [
        "1. **Rotatory Positional Embedding:**\n",
        "\n",
        "Implementing Rotary Positional Embeddings involves modifying the way positional embeddings are generated in the model. This method follows the RoFormer paper by Su et al. Instead of using sine and cosine positional encodings, the rotational positional embeddings are calculated using sinusoidal functions in a different manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGv8N3OTbL8l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RotaryPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super(RotaryPositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.rotary_dims = 64  # Number of dimensions for rotary embedding\n",
        "\n",
        "        # Create a matrix for rotational embeddings\n",
        "        self.rotary_emb = nn.Parameter(torch.randn(self.max_len, self.rotary_dims // 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        assert seq_len <= self.max_len, \"Input sequence length exceeds maximum sequence length.\"\n",
        "\n",
        "        # Extract embeddings and apply rotational embeddings\n",
        "        positional_emb = torch.arange(seq_len).expand(x.size(0), seq_len).to(x.device)\n",
        "        sinusoid_inp = positional_emb.unsqueeze(-1) / (10000 ** (torch.arange(0, self.d_model, 2) / self.d_model))\n",
        "\n",
        "        sin_emb = torch.sin(sinusoid_inp[:, :, 0::2])\n",
        "        cos_emb = torch.cos(sinusoid_inp[:, :, 1::2])\n",
        "\n",
        "        # Apply rotation to positional embeddings\n",
        "        # Assuming x is the original token embeddings\n",
        "        rot_embeddings = torch.cat((sin_emb, cos_emb), dim=-1)\n",
        "        rot_embeddings = torch.matmul(rot_embeddings, self.rotary_emb.to(rot_embeddings.device).transpose(0, 1))\n",
        "\n",
        "        # Combine token embeddings with rotary positional embeddings\n",
        "        output = x + rot_embeddings.unsqueeze(0)\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "seq_length = 50\n",
        "batch_size = 16\n",
        "vocab_size = 10000\n",
        "embedding_size = 256\n",
        "\n",
        "# Create input tensor\n",
        "input_data = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
        "embedding = nn.Embedding(vocab_size, embedding_size)(input_data)\n",
        "\n",
        "# Apply Rotary Positional Encoding\n",
        "rotary_positional_encoding = RotaryPositionalEncoding(embedding_size)\n",
        "output = rotary_positional_encoding(embedding)\n",
        "\n",
        "print(output.shape)  # Check the output shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn0PqihFmib0"
      },
      "source": [
        "2. **Group Query Attention:**\n",
        "\n",
        "\n",
        "Implementing Group Query Attention involves modifying the standard attention mechanism in the transformer architecture to accommodate group queries for enhanced information retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_Hg-yBkm7sz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GroupQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, group_size):\n",
        "        super(GroupQueryAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.group_size = group_size\n",
        "\n",
        "        # Initialize linear layers for queries, keys, and values\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Additional linear layer for grouping queries\n",
        "        self.group_query = nn.Linear(d_model, num_heads * group_size)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.num_heads, x.size(-1) // self.num_heads)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Generate grouped queries\n",
        "        group_q = self.group_query(q).view(batch_size, self.num_heads, self.group_size, -1)\n",
        "\n",
        "        # Apply linear transformations for queries, keys, and values\n",
        "        q = self.split_heads(self.query(q), batch_size)\n",
        "        k = self.split_heads(self.key(k), batch_size)\n",
        "        v = self.split_heads(self.value(v), batch_size)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(group_q.unsqueeze(-2), k.transpose(-2, -1)) / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
        "        scores = scores.squeeze(-2)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention.unsqueeze(-2), v).view(batch_size, self.num_heads, self.group_size, -1)\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, k.size(-1))\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "seq_length = 50\n",
        "batch_size = 16\n",
        "embedding_size = 256\n",
        "num_heads = 8\n",
        "group_size = 4\n",
        "\n",
        "# Create input tensors\n",
        "q = torch.randn(batch_size, seq_length, embedding_size)\n",
        "k = torch.randn(batch_size, seq_length, embedding_size)\n",
        "v = torch.randn(batch_size, seq_length, embedding_size)\n",
        "\n",
        "# Apply Group Query Attention\n",
        "group_query_attention = GroupQueryAttention(embedding_size, num_heads, group_size)\n",
        "output = group_query_attention(q, k, v)\n",
        "\n",
        "print(output.shape)  # Check the output shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUSME2jXnOOq"
      },
      "source": [
        "3. **Sliding Window Attention:**\n",
        "\n",
        "\n",
        "Implementing Sliding Window Attention involves modifying the standard attention mechanism in the transformer architecture to incorporate a sliding window approach for more efficient attention computation over long sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSuulLsJnXBH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SlidingWindowAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, window_size):\n",
        "        super(SlidingWindowAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        # Initialize linear layers for queries, keys, and values\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.num_heads, x.size(-1) // self.num_heads)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "        seq_length = q.size(1)\n",
        "\n",
        "        # Calculate sliding windows\n",
        "        windows = [(i, min(i + self.window_size, seq_length)) for i in range(0, seq_length, self.window_size)]\n",
        "\n",
        "        # Initialize outputs\n",
        "        outputs = []\n",
        "\n",
        "        for start, end in windows:\n",
        "            # Slice the input sequences within the window\n",
        "            q_window = q[:, start:end, :]\n",
        "            k_window = k[:, start:end, :]\n",
        "            v_window = v[:, start:end, :]\n",
        "\n",
        "            # Apply linear transformations for queries, keys, and values\n",
        "            q_window = self.split_heads(self.query(q_window), batch_size)\n",
        "            k_window = self.split_heads(self.key(k_window), batch_size)\n",
        "            v_window = self.split_heads(self.value(v_window), batch_size)\n",
        "\n",
        "            # Compute attention scores for the window\n",
        "            scores = torch.matmul(q_window, k_window.transpose(-2, -1)) / torch.sqrt(torch.tensor(k_window.size(-1), dtype=torch.float32))\n",
        "\n",
        "            if mask is not None:\n",
        "                scores = scores.masked_fill(mask[:, start:end, start:end] == 0, float('-inf'))\n",
        "\n",
        "            attention = F.softmax(scores, dim=-1)\n",
        "            output = torch.matmul(attention, v_window)\n",
        "            outputs.append(output)\n",
        "\n",
        "        # Concatenate outputs from different windows\n",
        "        output = torch.cat(outputs, dim=1)\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "seq_length = 1000  # Example sequence length\n",
        "batch_size = 16\n",
        "embedding_size = 256\n",
        "num_heads = 8\n",
        "window_size = 128  # Define window size\n",
        "\n",
        "# Create input tensors\n",
        "q = torch.randn(batch_size, seq_length, embedding_size)\n",
        "k = torch.randn(batch_size, seq_length, embedding_size)\n",
        "v = torch.randn(batch_size, seq_length, embedding_size)\n",
        "\n",
        "# Apply Sliding Window Attention\n",
        "sliding_window_attention = SlidingWindowAttention(embedding_size, num_heads, window_size)\n",
        "output = sliding_window_attention(q, k, v)\n",
        "\n",
        "print(output.shape)  # Check the output shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juaGkPbxnrbW"
      },
      "source": [
        "# Task 3: Training Loop Implementation\n",
        "\n",
        "Finally, create a training loop considering these following requirements:\n",
        "\n",
        "1. **Single GPU Training Loop:** Your base implementation should be equipped to train your model on a single GPU setup.\n",
        "2. **Distributed Data Parallel (DDP):** Extend your single GPU training loop to support training across multiple GPUs using DDP. Revisit the [PyTorch's DDP tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) for guidance.\n",
        "3. **Fully Sharded Data Parallel (FSDP):** Implement FSDP as a part of your training loop to shard the model parameters, gradients, and optimizer state. You can follow [Gupta et al., 2020, Training GPT-3 Like Models on a Single Machine](https://arxiv.org/pdf/2101.06840.pdf) for a comprehensive understanding of it.\n",
        "\n",
        "**Deliverable:** A Python script containing a functional training loop that is compatible with single GPU, DDP, and FSDP options along with a documentation illustrating how the code adapts to each setting.\n",
        "\n",
        "**Evaluation Scheme:** Each feature implementation will account for:\n",
        "\n",
        "- Single GPU: 10 points\n",
        "- DDP: 10 points\n",
        "- FSDP: 20 points\n",
        "\n",
        "**Note:** Document your code, approaches, difficulties encountered, and your solutions\n",
        "thoroughly. Include any reference materials you used in your report. Focus on clear communication of your methodologies and results.\n",
        "\n",
        "**Submission:**\n",
        "\n",
        "For each subtask, submit your source code and a brief description of your implementations. If relevant, please support your findings with visualizations of the alterations and their impacts.\n",
        "\n",
        "Please remember, partial points will be awarded for each part, so it's better to submit an incomplete assignment than no assignment at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G49qhBPn6Xs"
      },
      "source": [
        "1. **Single GPU Training Loop:**\n",
        "\n",
        "Training a deep learning model on a single GPU involves setting up the training loop, handling data loading, optimizer updates, and logging. Here's an explanation and a simplified code snippet illustrating a training loop for a model using a single GPU in PyTorch:\n",
        "\n",
        "* **Device Assignment:** Set the device to the GPU for computation.\n",
        "* **Data Loading:** Prepare your dataset and data loaders.\n",
        "* **Model Initialization:** Instantiate your model and move it to the GPU.\n",
        "* **Loss Function and Optimizer:** Define the loss function and optimizer.\n",
        "* **Training Loop:** Iterate through batches, perform forward pass, calculate loss, backward pass (gradient computation), and optimizer step.\n",
        "* **Logging and Monitoring:** Optionally, log and monitor training metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBt1kn6rn4LT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Define your model architecture (replace this with your model)\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(YourModel, self).__init__()\n",
        "        # Define your model layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implement forward pass\n",
        "        return x\n",
        "\n",
        "# Create a dummy dataset and DataLoader (replace this with your dataset)\n",
        "input_data = torch.randn(1000, 10)  # Example input data\n",
        "target_data = torch.randint(0, 2, (1000,))  # Example target data\n",
        "\n",
        "dataset = TensorDataset(input_data, target_data)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define your model, loss function, and optimizer\n",
        "model = Model().cuda()  # Move model to GPU\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model in training mode\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for inputs, targets in data_loader:\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()  # Move inputs and targets to GPU\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss /= len(dataset)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Optionally, save the trained model\n",
        "torch.save(model.state_dict(), 'my_model.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7lgo1C7pteL"
      },
      "source": [
        "2. **Distributed Data Parallel(DDP):**\n",
        "\n",
        "Distributed Data Parallel (DDP) in PyTorch allows training models across multiple GPUs within a single machine or across multiple machines. It splits the mini-batches and scatters them across GPUs, computes gradients independently, and then synchronizes and updates the model parameters. Here's an explanation and an example code snippet illustrating how to use DDP for training across multiple GPUs:\n",
        "* **Initialize DDP Process Group:** Initialize the process group for DDP.\n",
        "* **Set Device and Model:** Assign the device to GPUs and move the model to each GPU.\n",
        "* **Wrap Model with DDP:** Wrap the model with torch.nn.parallel.DistributedDataParallel.\n",
        "* **Data Loading and Batch Splitting:** Modify the data loading to split the batches across GPUs using torch.utils.data.distributed.DistributedSampler.\n",
        "* **Training Loop:** Similar to the single GPU training loop, iterate through batches, perform forward pass, calculate loss, backward pass, and optimizer step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqaAPPMPqN1I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.parallel\n",
        "import torch.utils.data.distributed\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Define your model architecture (replace this with your model)\n",
        "class YourModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(YourModel, self).__init__()\n",
        "        # Define your model layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implement forward pass\n",
        "        return x\n",
        "\n",
        "def train(rank, world_size):\n",
        "    torch.manual_seed(42)\n",
        "    # Initialize the process group for DDP\n",
        "    dist.init_process_group(backend='nccl', init_method='tcp://127.0.0.1:23456', world_size=world_size, rank=rank)\n",
        "\n",
        "    # Set the device to GPU and assign model to device\n",
        "    device = torch.device(f'cuda:{rank}')\n",
        "    torch.cuda.set_device(device)\n",
        "\n",
        "    model = YourModel().to(device)\n",
        "\n",
        "    # Wrap model with DDP\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "\n",
        "    # Create a dummy dataset and DataLoader (replace this with your dataset)\n",
        "    input_data = torch.randn(1000, 10)  # Example input data\n",
        "    target_data = torch.randint(0, 2, (1000,))  # Example target data\n",
        "\n",
        "    dataset = TensorDataset(input_data, target_data)\n",
        "    sampler = torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
        "    data_loader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
        "\n",
        "    # Define your loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model in training mode\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss /= len(dataset)\n",
        "        print(f\"Rank {rank} - Epoch [{epoch + 1}/{num_epochs}] Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    world_size = torch.cuda.device_count()\n",
        "    mp.spawn(train, args=(world_size,), nprocs=world_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShJQCDk7qRK3"
      },
      "source": [
        "3. **Fully Sharded Data Parallel (FSDP):**\n",
        "\n",
        "Fully Sharded Data Parallel (FSDP) is a technique designed to shard model parameters, optimizer state, and gradients across multiple devices or GPUs. It allows training very large models that may not fit on a single GPU or device by distributing the model's components across multiple devices.\n",
        "* **harding Model Parameters:** Splitting model parameters into shards and distributing them across devices.\n",
        "* **Sharding Gradients and Optimizer State:** Similar to model parameters, gradients and optimizer states are sharded across devices.\n",
        "* **Local SGD and Gradient Accumulation:** FSDP often employs Local SGD (Stochastic Gradient Descent) for independent updates on each shard, and gradient accumulation to synchronize gradients across shards.\n",
        "* **Optimizer Modifications:** Adjustments in the optimizer for FSDP, such as applying gradient scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vKzrLzGq9f2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch_optimizer as optim  # For use with FSDP\n",
        "\n",
        "# Define your model architecture (replace this with your model)\n",
        "class YourModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(YourModel, self).__init__()\n",
        "        # Define your model layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implement forward pass\n",
        "        return x\n",
        "\n",
        "def train(rank, world_size):\n",
        "    # Set the device to GPU and assign model to device\n",
        "    device = torch.device(f'cuda:{rank}')\n",
        "    torch.cuda.set_device(device)\n",
        "\n",
        "    model = YourModel().to(device)\n",
        "\n",
        "    # Wrap model with FSDP\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "\n",
        "    # Create a dummy dataset and DataLoader (replace this with your dataset)\n",
        "    input_data = torch.randn(1000, 10)  # Example input data\n",
        "    target_data = torch.randint(0, 2, (1000,))  # Example target data\n",
        "\n",
        "    dataset = TensorDataset(input_data, target_data)\n",
        "    sampler = torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
        "    data_loader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
        "\n",
        "    # Define your loss function and optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.DiffGrad(model.parameters(), lr=0.001)  # Use FSDP-compatible optimizer\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model in training mode\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss /= len(dataset)\n",
        "        print(f\"Rank {rank} - Epoch [{epoch + 1}/{num_epochs}] Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    world_size = torch.cuda.device_count()\n",
        "    mp.spawn(train, args=(world_size,), nprocs=world_size)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
